{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinalModel_RoBerTA.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPMToAix+9OsYWGEUiIzwMw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"efb4bde734fb495da0ae34f6cbd0aced":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be850f6b09a6493b87675ad4838441e5","IPY_MODEL_465d7b379ced491082cbd0e5407a67d8","IPY_MODEL_5b6bffb6b7c74b3181b9120df2c531ea"],"layout":"IPY_MODEL_f3142c9ed0bb4a6781506dac94aa530b"}},"be850f6b09a6493b87675ad4838441e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e211d2a784c442098ba3f687de566b6a","placeholder":"​","style":"IPY_MODEL_7e4a588c58cf430abafb4c1165d54e5a","value":"Downloading: 100%"}},"465d7b379ced491082cbd0e5407a67d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4be422ce49b34fd588b3fb6d7267b772","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f89ffedca0ec4b7b8f1db254d6e86ba6","value":898823}},"5b6bffb6b7c74b3181b9120df2c531ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f41892e6f27842598373b5f8d8137d75","placeholder":"​","style":"IPY_MODEL_29c5104305384e83a30752962e8106b2","value":" 878k/878k [00:00&lt;00:00, 1.26MB/s]"}},"f3142c9ed0bb4a6781506dac94aa530b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e211d2a784c442098ba3f687de566b6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e4a588c58cf430abafb4c1165d54e5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4be422ce49b34fd588b3fb6d7267b772":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f89ffedca0ec4b7b8f1db254d6e86ba6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f41892e6f27842598373b5f8d8137d75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c5104305384e83a30752962e8106b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edca74471fc8423e8a5684d6034f0272":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dcf2a07361a844b8a160a15474bdce90","IPY_MODEL_0bc4e951dd1e4d5d8b6860a0296a461e","IPY_MODEL_e1e65953d86f4105b7fb3820fd078041"],"layout":"IPY_MODEL_7e39adec39384ad3a9ea40ca766cf7ad"}},"dcf2a07361a844b8a160a15474bdce90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da30b6bd5014bbe8119603c21907188","placeholder":"​","style":"IPY_MODEL_d2e09a38ebb246e5a517adb2c180b84a","value":"Downloading: 100%"}},"0bc4e951dd1e4d5d8b6860a0296a461e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e5260211c524d529307a15bce63e70b","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7521ff426449443381f512702b6a3c02","value":456318}},"e1e65953d86f4105b7fb3820fd078041":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_366fbb5514d74df7a70bf8bc034cba2e","placeholder":"​","style":"IPY_MODEL_2c3ef6fa3239463a837bbee277f154f2","value":" 446k/446k [00:00&lt;00:00, 824kB/s]"}},"7e39adec39384ad3a9ea40ca766cf7ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7da30b6bd5014bbe8119603c21907188":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2e09a38ebb246e5a517adb2c180b84a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e5260211c524d529307a15bce63e70b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7521ff426449443381f512702b6a3c02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"366fbb5514d74df7a70bf8bc034cba2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c3ef6fa3239463a837bbee277f154f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2f9aa755de948928889e9bb898f5a93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a70f9206c0c4054bdc37539a4609c3e","IPY_MODEL_2d86bc3ad85b44cba233ef5b400615ef","IPY_MODEL_21fdb7fbbf0b4e49aa5d95cbb32a610b"],"layout":"IPY_MODEL_8ddee6fd5cc54734a1744c3db8b5a812"}},"2a70f9206c0c4054bdc37539a4609c3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9619bb663a8f4645b9f882e3e26f1626","placeholder":"​","style":"IPY_MODEL_4555dd86c3c6424db6599f5f85b57c7c","value":"Downloading: 100%"}},"2d86bc3ad85b44cba233ef5b400615ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_70e62e5c5bd94e29ae9848352c798b9e","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fbe04474df064325ad7a0e4fb0d5fb3e","value":481}},"21fdb7fbbf0b4e49aa5d95cbb32a610b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2137d906521747e690d55db9fb7eb50c","placeholder":"​","style":"IPY_MODEL_f66754274ce84310be337031563a7b01","value":" 481/481 [00:00&lt;00:00, 3.40kB/s]"}},"8ddee6fd5cc54734a1744c3db8b5a812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9619bb663a8f4645b9f882e3e26f1626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4555dd86c3c6424db6599f5f85b57c7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70e62e5c5bd94e29ae9848352c798b9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbe04474df064325ad7a0e4fb0d5fb3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2137d906521747e690d55db9fb7eb50c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f66754274ce84310be337031563a7b01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb3f43e42abc4a38a61bca9737e9a8d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c42e32adb5304501a0afcdd7fdc6c9ea","IPY_MODEL_171f4703000745cdbd76a7a1700ef889","IPY_MODEL_4e1e76f8321e41ac94a76eb383b91abb"],"layout":"IPY_MODEL_68c29503c1bf4a88b7c05f0d90613e1f"}},"c42e32adb5304501a0afcdd7fdc6c9ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c788bce6374c423f9a375b989dfb576d","placeholder":"​","style":"IPY_MODEL_fd88534e960a480a9acf30536a2da534","value":"Downloading: 100%"}},"171f4703000745cdbd76a7a1700ef889":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_546cfb291fbc42dc85a81141c211a00a","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fa87aacf9b74f7299bda17ac73f3d64","value":501200538}},"4e1e76f8321e41ac94a76eb383b91abb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53cdcce0f0f546c8b2ec478c0a818aa6","placeholder":"​","style":"IPY_MODEL_5a13d762263a443bb996e3a264db0749","value":" 478M/478M [00:14&lt;00:00, 37.2MB/s]"}},"68c29503c1bf4a88b7c05f0d90613e1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c788bce6374c423f9a375b989dfb576d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd88534e960a480a9acf30536a2da534":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"546cfb291fbc42dc85a81141c211a00a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa87aacf9b74f7299bda17ac73f3d64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53cdcce0f0f546c8b2ec478c0a818aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a13d762263a443bb996e3a264db0749":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["###Setup"],"metadata":{"id":"qZ1aakvS6TpO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-cEPEEFISqr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649505424606,"user_tz":-480,"elapsed":14901,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"95fb01c9-d23f-4d89-e7cd-4d5e8f697fbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!rm -f project1\n","!ln -s \"/content/drive/MyDrive/Colab Notebooks/COMP4332/Project 1\" project1"],"metadata":{"id":"c3J8oR5BijYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -f data\n","!ln -s \"/content/drive/MyDrive/Colab Notebooks/COMP4332/Project 1/data\" data"],"metadata":{"id":"us-VJwx2Ej81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cat project1/FinalModel/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3z15GEO08QHi","executionInfo":{"status":"ok","timestamp":1649484702120,"user_tz":-480,"elapsed":1643,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"ecc3c1d3-9ba5-4f4c-c0fa-9834ad5e72cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tqdm\n","torch>=1.7.0\n","transformers>=4.15.0\n","tensorflow-gpu>=2.0.0\n","nltk\n","scikit-learn\n","absl-py\n","pandas"]}]},{"cell_type":"code","source":["!pip install -r project1/FinalModel/requirements.txt"],"metadata":{"id":"IcRtTRyt8kLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python project1/FinalModel/main.py --help"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tB_xWOck9HYi","executionInfo":{"status":"ok","timestamp":1649485170884,"user_tz":-480,"elapsed":10601,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"d7ac19f6-88b9-4faa-ef5e-74ea1795d501"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","\n","A lot of the codes are inspired from the following blog:\n","https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n","\n","flags:\n","\n","project1/FinalModel/main.py:\n","  --batch_size: batch size: 16 or 32 preferred\n","    (default: '16')\n","    (an integer)\n","  --data_path: data directory path\n","    (default: 'data')\n","  --dropout: dropout rate\n","    (default: '0.3')\n","    (a number)\n","  --epochs: number of training epochs\n","    (default: '3')\n","    (an integer)\n","  --eval_every: number of training steps after each the model is evaluated\n","    (default: '50')\n","    (an integer)\n","  --lp_step: number of Linear Probing\n","    (default: '100')\n","    (an integer)\n","  --lr: learning rate. Preferred 2e-5, 3e-5, 5e-5\n","    (default: '2e-05')\n","    (a number)\n","  --max_len: max sentence length. max value is 512 for bert\n","    (default: '256')\n","    (an integer)\n","  --model_name: which transformer to use\n","    (default: 'bert-base-cased')\n","  --other_features: other feature aggregations to use\n","    (default: '')\n","    (a comma separated list)\n","  --other_hidden_dim: hidden dim for other features\n","    (default: '10')\n","    (an integer)\n","  --save_path: where to save the model\n","    (default: 'models/{}_bs{}_lr{}_drop{}_hidden{}_seed{}.pth')\n","  --seed: to reproduce the experiment\n","    (default: '101')\n","    (an integer)\n","  --use_lpft: whether to apply the method of Linear Probing and Finetuning. If\n","    True, in the lp_step steps, only train classifier head, after that finetune\n","    the whole model.\n","    (default: '0')\n","    (an integer)\n","  --[no]use_pooled: whether to use pooled output of Bert\n","    (default: 'true')\n","  --use_uncased: help to experiment with RoBERTa uncased\n","    (default: '0')\n","    (an integer)\n","\n","Try --helpfull to get a list of all flags.\n"]}]},{"cell_type":"markdown","source":["Above is general info for the model"],"metadata":{"id":"kKzoV6q-_lLg"}},{"cell_type":"markdown","source":["###Preprocess"],"metadata":{"id":"b10UMOpn_4rZ"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import os\n","import numpy as np\n","\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","\n","stopwords = set(stopwords.words('english'))\n","ps = PorterStemmer()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KiDiwvMC_6_V","executionInfo":{"status":"ok","timestamp":1649505501911,"user_tz":-480,"elapsed":1550,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"0a27e12a-5b1e-4cf3-b5a3-29192e373d62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["def lower(s):\n","    \"\"\"\n","    :param s: a string.\n","    return a string with lower characters\n","    Note that we allow the input to be nested string of a list.\n","    e.g.\n","    Input: 'Text mining is to identify useful information.'\n","    Output: 'text mining is to identify useful information.'\n","    \"\"\"\n","    if isinstance(s, list):\n","        return [lower(t) for t in s]\n","    if isinstance(s, str):\n","        return s.lower()\n","    else:\n","        raise NotImplementedError(\"unknown datatype\")\n","\n","\n","def tokenize(text):\n","    \"\"\"\n","    :param text: a doc with multiple sentences, type: str\n","    return a word list, type: list\n","    e.g.\n","    Input: 'Text mining is to identify useful information.'\n","    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n","    \"\"\"\n","    return nltk.word_tokenize(text)\n","\n","\n","def stem(tokens):\n","    \"\"\"\n","    :param tokens: a list of tokens, type: list\n","    return a list of stemmed words, type: list\n","    e.g.\n","    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n","    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n","    \"\"\"\n","    ### equivalent code\n","    # results = list()\n","    # for token in tokens:\n","    #     results.append(ps.stem(token))\n","    # return results\n","\n","    return [ps.stem(token) for token in tokens]\n","\n","\n","def n_gram(tokens, n=1):\n","    \"\"\"\n","    :param tokens: a list of tokens, type: list\n","    :param n: the corresponding n-gram, type: int\n","    return a list of n-gram tokens, type: list\n","    e.g.\n","    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n","    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n","    \"\"\"\n","    if n == 1:\n","        return tokens\n","    else:\n","        results = list()\n","        for i in range(len(tokens) - n + 1):\n","            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n","            results.append(\" \".join(tokens[i:i + n]))\n","        return results\n","\n","\n","def filter_stopwords(tokens):\n","    \"\"\"\n","    :param tokens: a list of tokens, type: list\n","    return a list of filtered tokens, type: list\n","    e.g.\n","    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n","    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n","    \"\"\"\n","    ### equivalent code\n","    # results = list()\n","    # for token in tokens:\n","    #     if token not in stopwords and not token.isnumeric():\n","    #         results.append(token)\n","    # return results\n","\n","    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n","\n","\n","def get_pretrained_embedding(file_path, tokenizer, embedding_dim):\n","    if not os.path.exists(file_path):\n","        return None\n","    embeddings_index = {}\n","    with open(file_path) as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","\n","    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n","    for word, i in tokenizer.word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            # words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector\n","\n","    return embedding_matrix"],"metadata":{"id":"Am0Emi-qAGVl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Dataset"],"metadata":{"id":"gctgYPMM6XwM"}},{"cell_type":"code","source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.utils.class_weight import compute_class_weight\n","import torch\n","from transformers import BertTokenizer, RobertaTokenizer, XLNetTokenizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","#from preprocess import *"],"metadata":{"id":"mnTd-mnA611z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_dataloader(root,\n","                      mode,\n","                      model_name,\n","                      batch_size=32,\n","                      max_length=256,\n","                      columns=[\"cool\", \"funny\", \"useful\"],\n","                      use_uncased=False):\n","    review_ds = SentimentDataset(root,\n","                                 mode,\n","                                 model_name,\n","                                 max_length=max_length,\n","                                 columns=columns,\n","                                 use_uncased=use_uncased)\n","\n","    # shuffle the dataset if it is not test dataset\n","    dataloader = torch.utils.data.DataLoader(review_ds,\n","                                             batch_size=batch_size,\n","                                             shuffle=mode == \"train\")\n","\n","    class_weights = review_ds.get_class_weights()\n","\n","    return dataloader, class_weights\n","\n","\n","class SentimentDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self,\n","                 root,\n","                 mode,\n","                 model_name,\n","                 framework=\"pt\",\n","                 max_length=256,\n","                 columns=[\"cool\", \"funny\", \"useful\"],\n","                 tokenizer=None,\n","                 use_uncased=False):\n","        self.root = root\n","        self.mode = mode\n","        self.data_file = pd.read_csv(os.path.join(self.root, f\"{self.mode}.csv\"))\n","        self.framework = framework\n","        if use_uncased:\n","            self.data_file['text'] = self.data_file['text'].map(lower)\n","\n","        self.review_texts = None\n","        if model_name == \"lstm-cnn\":\n","            '''self.review_texts = self.data_file[\"text\"].map(lower).map(tokenize).map(stem)\n","            if mode != \"train\":\n","                assert tokenizer is not None\n","                assert isinstance(tokenizer, Tokenizer)\n","                self.tokenizer = tokenizer\n","            else:\n","                self.tokenizer = Tokenizer(split=' ', oov_token=\"[OOV]\")\n","                self.tokenizer.fit_on_texts(self.review_texts)''' #discard LSTM\n","        else:\n","            if \"roberta\" in model_name:\n","                tokenizer_base = RobertaTokenizer\n","            elif \"bert\" in model_name:\n","                tokenizer_base = BertTokenizer\n","            elif \"xlnet\" in model_name:\n","                tokenizer_base = XLNetTokenizer\n","            else:\n","                raise NotImplementedError\n","            self.tokenizer = tokenizer_base.from_pretrained(model_name)\n","        self.max_length = max_length\n","\n","        if self.review_texts is None:\n","            self.review_texts = self.data_file[\"text\"].to_list()\n","        if mode != \"test\":\n","            self.stars = self.data_file[\"stars\"].to_numpy()\n","            self.stars -= 1  # 1~5 -> 0~4\n","\n","        if len(columns) == 0:\n","            self.other_features = None\n","            return\n","\n","        # normalize other features to 0~1\n","        self.other_features = MinMaxScaler().fit_transform(\n","            self.data_file[columns].to_numpy())\n","\n","    def __len__(self):\n","        return len(self.review_texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.review_texts[idx]\n","        if self.mode != \"test\":\n","            label = self.stars[idx]\n","\n","        encoded = self.tokenizer.encode_plus(text,\n","                                             add_special_tokens=True,\n","                                             max_length=self.max_length,\n","                                             return_token_type_ids=False,\n","                                             padding='max_length',\n","                                             return_attention_mask=True,\n","                                             return_tensors=self.framework,\n","                                             truncation=True)\n","\n","        data = {\n","            \"input_ids\": encoded[\"input_ids\"][0],\n","            \"attention_mask\": encoded[\"attention_mask\"][0]\n","        }\n","        if self.mode != \"test\":\n","            data[\"label\"] = label\n","\n","        if self.other_features is not None:\n","            data[\"features\"] = torch.FloatTensor(self.other_features[idx])\n","        return data\n","\n","    def get_class_weights(self):\n","        if self.mode == \"test\":\n","            return None\n","        return compute_class_weight('balanced',\n","                                    classes=np.unique(self.stars),\n","                                    y=self.stars)\n","\n","    def get_keras_data(self):\n","        data = self.tokenizer.texts_to_sequences(self.review_texts)\n","        data = [pad_sequences(data, maxlen=self.max_length), self.other_features]\n","\n","        return data, self.stars"],"metadata":{"id":"wFOGOUwl6_RI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Model"],"metadata":{"id":"vcX-kduFATB3"}},{"cell_type":"code","source":["from transformers import BertModel, RobertaModel, XLNetModel\n","import torch\n","from torch import nn\n","from tensorflow import keras\n","import tensorflow as tf"],"metadata":{"id":"_0BG8-KeAUeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerSentimentAnalyzer(nn.Module):\n","\n","    def __init__(self,\n","                 model_name,\n","                 num_class=5,\n","                 num_other_features=3,\n","                 hidden_size=10,\n","                 dropout_rate=0.3,\n","                 use_pooled=True):\n","        super().__init__()\n","        self.use_pooled = use_pooled\n","\n","        if \"roberta\" in model_name:\n","            transformer_base = RobertaModel\n","        elif \"bert\" in model_name:\n","            transformer_base = BertModel\n","        elif \"xlnet\" in model_name:\n","            transformer_base = XLNetModel\n","            self.use_pooled = False  # no pooler for xlnet\n","\n","        self.transformer = transformer_base.from_pretrained(model_name)\n","        if not self.use_pooled:\n","            self.hidden = nn.Linear(self.transformer.config.hidden_size,\n","                                    self.transformer.config.hidden_size)\n","            nn.init.xavier_uniform_(self.hidden.weight, gain=nn.init.calculate_gain('relu'))\n","\n","        if num_other_features > 0:\n","            self.fc1 = nn.Linear(num_other_features, hidden_size)\n","            nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n","            self.other_relu = nn.ReLU()\n","            self.classifier = nn.Linear(self.transformer.config.hidden_size + hidden_size,\n","                                        num_class)\n","        else:\n","            self.classifier = nn.Linear(self.transformer.config.hidden_size, num_class)\n","\n","        nn.init.xavier_uniform_(self.classifier.weight)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","\n","    def forward(self, input_ids, attention_mask, other_features): #not used\n","        transformer_out = self.transformer(input_ids=input_ids,\n","                                           attention_mask=attention_mask)\n","        if self.use_pooled:\n","            output = transformer_out[\"pooler_output\"]\n","        else:\n","            cls_token = transformer_out[\"last_hidden_state\"][:, 0]  # get the [CLS] token\n","            output = self.hidden(cls_token)\n","        dropped = self.dropout(output)  # [batch_size, 768]\n","\n","        if hasattr(self, \"fc1\"):\n","            feat = self.fc1(other_features)  # [batch_size, num_other_features]\n","            feat = self.other_relu(feat)\n","            final = torch.cat([dropped, feat], axis=1)\n","        else:\n","            final = dropped\n","        return self.classifier(final)\n","\n","    def count_parameters(self):\n","        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n","\n","    def fix_transformer_stem(self, yes=True):\n","        if yes:\n","            for param in self.transformer.parameters():\n","                param.requires_grad = False\n","            print(f'Fixed Transformer stem. Total head trainable parameters {self.count_parameters()}')\n","        else:\n","            for param in self.transformer.parameters():\n","                param.requires_grad = True\n","            print(f'Trained Transformer stem. Total head trainable parameters {self.count_parameters()}')"],"metadata":{"id":"WvL9jzk2BTU9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Main\n","A lot of the codes are inspired from the following blog:\n","https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n"],"metadata":{"id":"UIOIqu1B5uGU"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import random\n","\n","from absl import flags\n","from absl import app\n","from sklearn.metrics import classification_report, confusion_matrix\n","import torch\n","from torch import nn\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","\n","#from model import TransformerSentimentAnalyzer\n","#from dataset import create_dataloader"],"metadata":{"id":"46uAxNW15wHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for name in list(flags.FLAGS):\n","  delattr(flags.FLAGS, name)"],"metadata":{"id":"16Tnj8waFyz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flags.DEFINE_string(\"model_name\", \"bert-base-cased\", \"which transformer to use\")\n","flags.DEFINE_integer(\"batch_size\", 16, \"batch size: 16 or 32 preferred\")\n","flags.DEFINE_integer(\"max_len\", 256, \"max sentence length. max value is 512 for bert\")\n","flags.DEFINE_bool(\"use_pooled\", True, \"whether to use pooled output of Bert\")\n","flags.DEFINE_integer(\"other_hidden_dim\", 10, \"hidden dim for other features\")\n","\n","flags.DEFINE_integer(\"epochs\", 3, \"number of training epochs\")\n","flags.DEFINE_integer(\"eval_every\", 50, \"number of training steps after each the model is evaluated\")\n","flags.DEFINE_float(\"lr\", 2e-5, \"learning rate. Preferred 2e-5, 3e-5, 5e-5\")\n","\n","flags.DEFINE_float(\"dropout\", 0.3, \"dropout rate\")\n","flags.DEFINE_list(\"other_features\", [],\n","                  \"other feature aggregations to use\")\n","\n","flags.DEFINE_string(\"data_path\", \"data\", \"data directory path\")\n","flags.DEFINE_string(\"save_path\", \"models/{}_bs{}_lr{}_drop{}_hidden{}_seed{}.pth\",\n","                    \"where to save the model\")\n","\n","flags.DEFINE_integer(\"use_uncased\", 0, \"help to experiment with RoBERTa uncased\")\n","flags.DEFINE_integer(\"use_lpft\", 0, \"whether to apply the method of Linear Probing and Finetuning. If True, in the lp_step steps, only train classifier head, after that finetune the whole model.\")\n","flags.DEFINE_integer(\"lp_step\", 100, \"number of Linear Probing\")\n","\n","flags.DEFINE_integer(\"seed\", 101, \"to reproduce the experiment\")\n","\n","FLAGS = flags.FLAGS\n","#HP\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"_20r1I1F6AdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, data_train, data_val, epochs, device, criterion, \n","          optimizer, scheduler, save_path, eval_every, use_lpft, lp_step):\n","    step = 0\n","    curr_best_val_f1_macro = 0\n","    best_val_at_step = 0\n","\n","    if use_lpft:\n","        model.fix_transformer_stem(True)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_bar = tqdm(data_train,\n","                         total=int(len(data_train)),\n","                         desc=f\"train: {epoch + 1} / {epochs}\")\n","\n","        correct_num = 0\n","        total_num = 0\n","        running_loss = 0\n","        for batch in train_bar:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            other_features = batch[\"features\"].to(device) if \"features\" in batch else None\n","            label = batch[\"label\"].to(device)\n","            step += 1\n","            logits = model(input_ids, attention_mask, other_features)\n","            predicted = torch.max(logits, dim=1)[1]\n","\n","            loss = criterion(logits, label)\n","            running_loss += loss.item()\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            correct_num += (predicted == label).sum().item()\n","            total_num += label.shape[0]\n","\n","            train_bar.set_postfix(acc=(correct_num / total_num),\n","                                  loss=(running_loss / total_num))\n","\n","            del batch, input_ids, attention_mask, other_features, label, logits, loss, predicted\n","\n","            if step == lp_step and use_lpft:\n","                model.fix_transformer_stem(False)\n","\n","            if step%eval_every == 0 or (step == lp_step and use_lpft):\n","                model.eval()\n","                y_pred = []\n","                y_true = []\n","                val_running_loss = 0\n","                \n","                with torch.no_grad():\n","                    for batch in data_val:\n","                        input_ids = batch[\"input_ids\"].to(device)\n","                        attention_mask = batch[\"attention_mask\"].to(device)\n","                        other_features = batch[\"features\"].to(\n","                            device) if \"features\" in batch else None\n","                        label = batch[\"label\"].to(device)\n","\n","                        logits = model(input_ids, attention_mask, other_features)\n","                        predicted = torch.max(logits, dim=1)[1]\n","\n","                        loss = criterion(logits, label)\n","                        val_running_loss += loss.item()\n","\n","                        y_pred.extend(predicted.tolist())\n","                        y_true.extend(label.tolist())\n","\n","                        del batch, input_ids, attention_mask, label, logits, predicted\n","                \n","                report = classification_report(y_true, y_pred, output_dict=True)\n","                print(\n","                    f\"[valid] epoch: {epoch}, global step: {step}, loss: {val_running_loss / len(data_val)},\"\n","                    f\" report:\\n{classification_report(y_true, y_pred, digits=4)}\"\n","                    f\"confusion_matrix:\\n{confusion_matrix(y_true, y_pred)}\"\n","                    )\n","\n","                if report['macro avg']['f1-score'] > curr_best_val_f1_macro:\n","                    curr_best_val_f1_macro = report[\"macro avg\"]['f1-score']\n","                    best_val_at_step = step\n","                    model_dir, name = save_path.rsplit(\"/\", 1)\n","                    # name = f\"acc{curr_best_val_f1_macro}_{name}\"\n","                    os.makedirs(model_dir, exist_ok=True)\n","                    torch.save(model.state_dict(), os.path.join(model_dir, name))\n","\n","        print(\n","            f\"[train] epoch: {epoch}, global step: {step}, loss: {running_loss / total_num},\"\n","            f\" accuracy: {correct_num / total_num}\")\n","\n","    print(f\"[finish] best valid macro avg is {curr_best_val_f1_macro}, achieved at global step {best_val_at_step}\")"],"metadata":{"id":"p1q0tgh8CDVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(list(FLAGS))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KT2fT0nGQBj","executionInfo":{"status":"ok","timestamp":1649487729552,"user_tz":-480,"elapsed":7,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"34d5db1a-f322-4d9c-bd82-49b6b429366f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['model_name', 'batch_size', 'max_len', 'use_pooled', 'other_hidden_dim', 'epochs', 'eval_every', 'lr', 'dropout', 'other_features', 'data_path', 'save_path', 'use_uncased', 'use_lpft', 'lp_step', 'seed']\n"]}]},{"cell_type":"code","source":["FLAGS.flag_values_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FoRU4cubIWbE","executionInfo":{"status":"ok","timestamp":1649498313663,"user_tz":-480,"elapsed":607,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"f14760fe-efd2-4261-fb81-ada36fe56254"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'batch_size': 16,\n"," 'data_path': 'data',\n"," 'dropout': 0.3,\n"," 'epochs': 3,\n"," 'eval_every': 50,\n"," 'lp_step': 100,\n"," 'lr': 2e-05,\n"," 'max_len': 256,\n"," 'model_name': 'bert-base-cased',\n"," 'other_features': [],\n"," 'other_hidden_dim': 10,\n"," 'save_path': 'models/{}_bs{}_lr{}_drop{}_hidden{}_seed{}.pth',\n"," 'seed': 101,\n"," 'use_lpft': 0,\n"," 'use_pooled': True,\n"," 'use_uncased': 0}"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["class parameters: #best set\n","  batch_size = 32\n","  data_path = 'data'\n","  dropout = 0.4\n","  epochs = 2\n","  eval_every = 100\n","  lp_step = 100\n","  lr = 1e-05\n","  max_len = 256\n","  model_name = 'roberta-base'\n","  other_features = []\n","  other_hidden_dim = 32\n","  save_path = 'project1/models/{}_bs{}_lr{}_drop{}_hidden{}_seed{}.pth'\n","  seed = 101\n","  use_lpft = 1\n","  use_pooled = False\n","  use_uncased = 0\n","\n","FLAGS = parameters"],"metadata":{"id":"HROzR73PKjUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = FLAGS.seed\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True"],"metadata":{"id":"hPsLbyuKCcUI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Load data\n","from dataset.py"],"metadata":{"id":"Nt5EDibPMyNR"}},{"cell_type":"code","source":["train_dataloader, class_weights = create_dataloader(FLAGS.data_path,\n","                                                        \"train\",\n","                                                        FLAGS.model_name,\n","                                                        batch_size=FLAGS.batch_size,\n","                                                        max_length=FLAGS.max_len,\n","                                                        columns=FLAGS.other_features,\n","                                                        use_uncased=FLAGS.use_uncased)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["efb4bde734fb495da0ae34f6cbd0aced","be850f6b09a6493b87675ad4838441e5","465d7b379ced491082cbd0e5407a67d8","5b6bffb6b7c74b3181b9120df2c531ea","f3142c9ed0bb4a6781506dac94aa530b","e211d2a784c442098ba3f687de566b6a","7e4a588c58cf430abafb4c1165d54e5a","4be422ce49b34fd588b3fb6d7267b772","f89ffedca0ec4b7b8f1db254d6e86ba6","f41892e6f27842598373b5f8d8137d75","29c5104305384e83a30752962e8106b2","edca74471fc8423e8a5684d6034f0272","dcf2a07361a844b8a160a15474bdce90","0bc4e951dd1e4d5d8b6860a0296a461e","e1e65953d86f4105b7fb3820fd078041","7e39adec39384ad3a9ea40ca766cf7ad","7da30b6bd5014bbe8119603c21907188","d2e09a38ebb246e5a517adb2c180b84a","1e5260211c524d529307a15bce63e70b","7521ff426449443381f512702b6a3c02","366fbb5514d74df7a70bf8bc034cba2e","2c3ef6fa3239463a837bbee277f154f2","c2f9aa755de948928889e9bb898f5a93","2a70f9206c0c4054bdc37539a4609c3e","2d86bc3ad85b44cba233ef5b400615ef","21fdb7fbbf0b4e49aa5d95cbb32a610b","8ddee6fd5cc54734a1744c3db8b5a812","9619bb663a8f4645b9f882e3e26f1626","4555dd86c3c6424db6599f5f85b57c7c","70e62e5c5bd94e29ae9848352c798b9e","fbe04474df064325ad7a0e4fb0d5fb3e","2137d906521747e690d55db9fb7eb50c","f66754274ce84310be337031563a7b01"]},"id":"_HW5u55xJXPn","executionInfo":{"status":"ok","timestamp":1649505529855,"user_tz":-480,"elapsed":3688,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"91413fc4-e7f6-4199-9fc7-9e3ed290f617"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb4bde734fb495da0ae34f6cbd0aced"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edca74471fc8423e8a5684d6034f0272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2f9aa755de948928889e9bb898f5a93"}},"metadata":{}}]},{"cell_type":"code","source":["print(class_weights) #balanced among #data per class"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59Lv1edmQpuq","executionInfo":{"status":"ok","timestamp":1649505529855,"user_tz":-480,"elapsed":4,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"e6dd26da-9637-4e1b-c8ab-7e180bc76b5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.34730539 2.47083047 1.80995475 0.90657265 0.45506257]\n"]}]},{"cell_type":"code","source":["type(train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qO5QJfz1RaOA","executionInfo":{"status":"ok","timestamp":1649505531599,"user_tz":-480,"elapsed":313,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"18179f04-7925-4b26-8260-9e9b238135ae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.utils.data.dataloader.DataLoader"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["len(train_dataloader) #an empty shell to load later later on"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g-ASECAgRe9w","executionInfo":{"status":"ok","timestamp":1649505532672,"user_tz":-480,"elapsed":3,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"34640fcd-0b5a-47b9-8648-bbe166894639"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["563"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["val_dataloader, _ = create_dataloader(FLAGS.data_path,\n","                                          \"valid\",\n","                                          FLAGS.model_name,\n","                                          batch_size=FLAGS.batch_size,\n","                                          max_length=FLAGS.max_len,\n","                                          columns=FLAGS.other_features,\n","                                          use_uncased=FLAGS.use_uncased)"],"metadata":{"id":"6eQWONicR-Kz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(val_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHzBx4S2SFWY","executionInfo":{"status":"ok","timestamp":1649505536005,"user_tz":-480,"elapsed":6,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"50e8e7e4-5ee6-41ae-c0b4-061469a7e4c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["####Build model\n","from model.py"],"metadata":{"id":"gSvYhV7ZSjqx"}},{"cell_type":"code","source":["model = TransformerSentimentAnalyzer(FLAGS.model_name,\n","                                         num_class=5,\n","                                         num_other_features=len(FLAGS.other_features),\n","                                         hidden_size=FLAGS.other_hidden_dim,\n","                                         dropout_rate=FLAGS.dropout,\n","                                         use_pooled=FLAGS.use_pooled).to(DEVICE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["bb3f43e42abc4a38a61bca9737e9a8d7","c42e32adb5304501a0afcdd7fdc6c9ea","171f4703000745cdbd76a7a1700ef889","4e1e76f8321e41ac94a76eb383b91abb","68c29503c1bf4a88b7c05f0d90613e1f","c788bce6374c423f9a375b989dfb576d","fd88534e960a480a9acf30536a2da534","546cfb291fbc42dc85a81141c211a00a","4fa87aacf9b74f7299bda17ac73f3d64","53cdcce0f0f546c8b2ec478c0a818aa6","5a13d762263a443bb996e3a264db0749"]},"id":"VTNBogIySMJz","executionInfo":{"status":"ok","timestamp":1649505567093,"user_tz":-480,"elapsed":27414,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"b278f300-bb52-4d0e-e6dd-82429b89c6be"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb3f43e42abc4a38a61bca9737e9a8d7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","source":["before training"],"metadata":{"id":"4oyoBk6TUSgo"}},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(DEVICE))\n","# loss_fn = nn.CrossEntropyLoss()\n","bert_optim = AdamW(model.parameters(), lr=FLAGS.lr, correct_bias=False)\n","\n","total_steps = len(train_dataloader) * FLAGS.epochs\n","scheduler = get_linear_schedule_with_warmup(bert_optim,\n","                                            num_warmup_steps=0,\n","                                            num_training_steps=total_steps)\n","\n","model_save_path = FLAGS.save_path.format(FLAGS.model_name, FLAGS.batch_size, FLAGS.lr,\n","                                         FLAGS.dropout, FLAGS.other_hidden_dim, FLAGS.seed)\n","\n","print(f'Fixed Transformer stem. Total head trainable parameters {model.count_parameters()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oa0-iH-2SQ7e","executionInfo":{"status":"ok","timestamp":1649505789793,"user_tz":-480,"elapsed":245,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"64ebd807-f518-4c8f-f675-b3d1ce435143"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fixed Transformer stem. Total head trainable parameters 125240069\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["DEVICE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHRjlEaow5qq","executionInfo":{"status":"ok","timestamp":1649505567094,"user_tz":-480,"elapsed":6,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"b1b0bfbc-2c3a-435b-c3f8-ed2e7120b839"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["train( ) from above"],"metadata":{"id":"K1v1JoCtLTc8"}},{"cell_type":"code","source":["with tf.device('/gpu:0'):\n","  history = train(model, train_dataloader, val_dataloader, FLAGS.epochs, DEVICE, loss_fn,\n","          bert_optim, scheduler, model_save_path,\n","          FLAGS.eval_every, FLAGS.use_lpft, FLAGS.lp_step)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZmH5hMu3VVzW","executionInfo":{"status":"ok","timestamp":1649509088405,"user_tz":-480,"elapsed":3294374,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"25b8b84f-83e3-4bea-81ac-5218f6d6bd24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fixed Transformer stem. Total head trainable parameters 594437\n"]},{"output_type":"stream","name":"stderr","text":["train: 1 / 2:  18%|█▊        | 99/563 [01:39<07:42,  1.00it/s, acc=0.231, loss=0.0564]"]},{"output_type":"stream","name":"stdout","text":["Trained Transformer stem. Total head trainable parameters 125240069\n","[valid] epoch: 0, global step: 100, loss: 1.5609858906458294, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.4649    0.7518    0.5745       282\n","           1     0.0000    0.0000    0.0000       136\n","           2     0.0000    0.0000    0.0000       212\n","           3     0.4043    0.0408    0.0741       466\n","           4     0.5426    0.8938    0.6753       904\n","\n","    accuracy                         0.5195      2000\n","   macro avg     0.2824    0.3373    0.2648      2000\n","weighted avg     0.4050    0.5195    0.4035      2000\n","confusion_matrix:\n","[[212   1   0   0  69]\n"," [ 66   0   1   8  61]\n"," [ 48   4   0  14 146]\n"," [ 41   1   0  19 405]\n"," [ 89   1   0   6 808]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 1 / 2:  35%|███▌      | 199/563 [06:46<14:54,  2.46s/it, acc=0.416, loss=0.0441]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 0, global step: 200, loss: 0.8684525546573457, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.7470    0.8794    0.8078       282\n","           1     0.6047    0.1912    0.2905       136\n","           2     0.4839    0.6368    0.5499       212\n","           3     0.4946    0.5858    0.5363       466\n","           4     0.8325    0.7312    0.7786       904\n","\n","    accuracy                         0.6715      2000\n","   macro avg     0.6325    0.6049    0.5926      2000\n","weighted avg     0.6893    0.6715    0.6688      2000\n","confusion_matrix:\n","[[248  12  16   5   1]\n"," [ 47  26  61   2   0]\n"," [ 18   4 135  50   5]\n"," [  8   0  58 273 127]\n"," [ 11   1   9 222 661]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 1 / 2:  53%|█████▎    | 299/563 [11:54<10:50,  2.46s/it, acc=0.495, loss=0.0382]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 0, global step: 300, loss: 0.8394207604347713, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.7778    0.8440    0.8095       282\n","           1     0.4464    0.5515    0.4934       136\n","           2     0.4252    0.5896    0.4941       212\n","           3     0.4559    0.5880    0.5136       466\n","           4     0.8938    0.6239    0.7349       904\n","\n","    accuracy                         0.6380      2000\n","   macro avg     0.5998    0.6394    0.6091      2000\n","weighted avg     0.6953    0.6380    0.6519      2000\n","confusion_matrix:\n","[[238  36   8   0   0]\n"," [ 37  75  23   1   0]\n"," [ 11  43 125  31   2]\n"," [  7   9 111 274  65]\n"," [ 13   5  27 295 564]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 1 / 2:  71%|███████   | 399/563 [17:02<06:44,  2.46s/it, acc=0.539, loss=0.0353]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 0, global step: 400, loss: 0.8408492817765191, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.8147    0.8262    0.8204       282\n","           1     0.4246    0.5588    0.4825       136\n","           2     0.5000    0.3962    0.4421       212\n","           3     0.5707    0.4850    0.5244       466\n","           4     0.8002    0.8595    0.8288       904\n","\n","    accuracy                         0.6980      2000\n","   macro avg     0.6220    0.6252    0.6196      2000\n","weighted avg     0.6914    0.6980    0.6921      2000\n","confusion_matrix:\n","[[233  39   9   1   0]\n"," [ 33  76  23   3   1]\n"," [ 10  55  84  55   8]\n"," [  5   6  44 226 185]\n"," [  5   3   8 111 777]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 1 / 2:  89%|████████▉ | 500/563 [23:10<21:30, 20.48s/it, acc=0.569, loss=0.0334]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 0, global step: 500, loss: 0.8189938910423763, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.8589    0.7553    0.8038       282\n","           1     0.4105    0.6912    0.5151       136\n","           2     0.4144    0.5708    0.4802       212\n","           3     0.4823    0.4957    0.4889       466\n","           4     0.8590    0.7146    0.7802       904\n","\n","    accuracy                         0.6525      2000\n","   macro avg     0.6050    0.6455    0.6136      2000\n","weighted avg     0.6936    0.6525    0.6658      2000\n","confusion_matrix:\n","[[213  63   6   0   0]\n"," [ 20  94  21   1   0]\n"," [  6  57 121  26   2]\n"," [  7   7 117 231 104]\n"," [  2   8  27 221 646]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 1 / 2: 100%|██████████| 563/563 [25:44<00:00,  2.74s/it, acc=0.581, loss=0.0326]\n"]},{"output_type":"stream","name":"stdout","text":["[train] epoch: 0, global step: 563, loss: 0.03256165544523133, accuracy: 0.581\n"]},{"output_type":"stream","name":"stderr","text":["train: 2 / 2:   6%|▋         | 36/563 [01:33<22:05,  2.51s/it, acc=0.669, loss=0.0262]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 1, global step: 600, loss: 0.827563931071569, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.8913    0.7270    0.8008       282\n","           1     0.3944    0.7279    0.5116       136\n","           2     0.5024    0.4858    0.4940       212\n","           3     0.5674    0.5150    0.5399       466\n","           4     0.8316    0.8197    0.8256       904\n","\n","    accuracy                         0.6940      2000\n","   macro avg     0.6374    0.6551    0.6344      2000\n","weighted avg     0.7139    0.6940    0.6991      2000\n","confusion_matrix:\n","[[205  70   6   1   0]\n"," [ 17  99  17   2   1]\n"," [  2  65 103  39   3]\n"," [  4   9  67 240 146]\n"," [  2   8  12 141 741]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 2 / 2:  24%|██▍       | 137/563 [07:40<2:25:32, 20.50s/it, acc=0.723, loss=0.0217]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 1, global step: 700, loss: 0.8383768390095423, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.9196    0.6489    0.7609       282\n","           1     0.3857    0.5956    0.4682       136\n","           2     0.5234    0.5802    0.5503       212\n","           3     0.4625    0.7275    0.5655       466\n","           4     0.8860    0.6106    0.7230       904\n","\n","    accuracy                         0.6390      2000\n","   macro avg     0.6354    0.6326    0.6136      2000\n","weighted avg     0.7196    0.6390    0.6560      2000\n","confusion_matrix:\n","[[183  88  10   1   0]\n"," [ 11  81  42   2   0]\n"," [  1  31 123  55   2]\n"," [  2   5  51 339  69]\n"," [  2   5   9 336 552]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 2 / 2:  42%|████▏     | 236/563 [11:46<13:24,  2.46s/it, acc=0.731, loss=0.0213]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 1, global step: 800, loss: 0.7998059642693353, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.8482    0.7730    0.8089       282\n","           1     0.4394    0.6397    0.5210       136\n","           2     0.5244    0.5566    0.5400       212\n","           3     0.5128    0.6009    0.5534       466\n","           4     0.8579    0.7345    0.7914       904\n","\n","    accuracy                         0.6835      2000\n","   macro avg     0.6366    0.6609    0.6429      2000\n","weighted avg     0.7123    0.6835    0.6934      2000\n","confusion_matrix:\n","[[218  58   6   0   0]\n"," [ 25  87  22   2   0]\n"," [  3  47 118  41   3]\n"," [  6   3  70 280 107]\n"," [  5   3   9 223 664]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 2 / 2:  60%|█████▉    | 336/563 [16:54<09:16,  2.45s/it, acc=0.733, loss=0.0212]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 1, global step: 900, loss: 0.7890727553102705, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.8421    0.7943    0.8175       282\n","           1     0.4450    0.6250    0.5199       136\n","           2     0.5275    0.5425    0.5349       212\n","           3     0.5144    0.6137    0.5597       466\n","           4     0.8596    0.7312    0.7902       904\n","\n","    accuracy                         0.6855      2000\n","   macro avg     0.6377    0.6613    0.6444      2000\n","weighted avg     0.7133    0.6855    0.6949      2000\n","confusion_matrix:\n","[[224  52   6   0   0]\n"," [ 27  85  22   2   0]\n"," [  4  47 115  44   2]\n"," [  6   4  64 286 106]\n"," [  5   3  11 224 661]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 2 / 2:  78%|███████▊  | 437/563 [23:01<43:01, 20.48s/it, acc=0.736, loss=0.0209]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 1, global step: 1000, loss: 0.8186110688580407, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.8919    0.7021    0.7857       282\n","           1     0.4105    0.5735    0.4785       136\n","           2     0.5160    0.6085    0.5584       212\n","           3     0.4934    0.6438    0.5587       466\n","           4     0.8658    0.6991    0.7736       904\n","\n","    accuracy                         0.6685      2000\n","   macro avg     0.6355    0.6454    0.6310      2000\n","weighted avg     0.7147    0.6685    0.6823      2000\n","confusion_matrix:\n","[[198  73  10   1   0]\n"," [ 16  78  39   3   0]\n"," [  2  31 129  48   2]\n"," [  4   4  62 300  96]\n"," [  2   4  10 256 632]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 2 / 2:  95%|█████████▌| 537/563 [28:07<08:52, 20.48s/it, acc=0.739, loss=0.0208]"]},{"output_type":"stream","name":"stdout","text":["[valid] epoch: 1, global step: 1100, loss: 0.7925672498014238, report:\n","              precision    recall  f1-score   support\n","\n","           0     0.8479    0.7908    0.8183       282\n","           1     0.4599    0.6324    0.5325       136\n","           2     0.5108    0.5566    0.5327       212\n","           3     0.5027    0.6009    0.5474       466\n","           4     0.8622    0.7268    0.7887       904\n","\n","    accuracy                         0.6820      2000\n","   macro avg     0.6367    0.6615    0.6439      2000\n","weighted avg     0.7118    0.6820    0.6921      2000\n","confusion_matrix:\n","[[223  52   6   1   0]\n"," [ 26  86  22   2   0]\n"," [  4  43 118  45   2]\n"," [  6   4  73 280 103]\n"," [  4   2  12 229 657]]\n"]},{"output_type":"stream","name":"stderr","text":["train: 2 / 2: 100%|██████████| 563/563 [29:09<00:00,  3.11s/it, acc=0.74, loss=0.0207]"]},{"output_type":"stream","name":"stdout","text":["[train] epoch: 1, global step: 1126, loss: 0.020699406385421754, accuracy: 0.7397222222222222\n","[finish] best valid macro avg is 0.6444327567697403, achieved at global step 900\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["model.count_parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abLiy64B4ijz","executionInfo":{"status":"ok","timestamp":1649509713435,"user_tz":-480,"elapsed":250,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"a57fffbf-8f91-4f1e-abeb-b14f0501cd72"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["125240069"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["model.eval()"],"metadata":{"id":"U52v-3XTfs7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Load best model"],"metadata":{"id":"k1lUf98mhYa-"}},{"cell_type":"code","source":["FLAGS.save_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"xzOaK9JNh0bZ","executionInfo":{"status":"ok","timestamp":1649511243429,"user_tz":-480,"elapsed":318,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"640119f4-1448-4d47-cc53-e506ff00154c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'project1/models/{}_bs{}_lr{}_drop{}_hidden{}_seed{}.pth'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["!ls project1/models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtXfghzuiIK0","executionInfo":{"status":"ok","timestamp":1649511474134,"user_tz":-480,"elapsed":313,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"4ecb5100-17d0-4936-a4dd-f158526b6fa8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["roberta-base_bs32_lr1e-05_drop0.4_hidden32_seed101.pth\n"]}]},{"cell_type":"code","source":["model_path = '/content/project1/models/roberta-base_bs32_lr1e-05_drop0.4_hidden32_seed101.pth'"],"metadata":{"id":"Wfa8J6RpiZLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load(model_path))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_INFYphhbFQ","executionInfo":{"status":"ok","timestamp":1649511552677,"user_tz":-480,"elapsed":751,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"64197dd0-18a2-48ae-c8ae-5c3753fddc65"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["####Evaluate.py"],"metadata":{"id":"4BLAwHhsWX7d"}},{"cell_type":"code","source":["def evaluate(model, test_data, device, mode=\"test\", save_name=\"pred.csv\"):\n","    test_bar = tqdm(test_data, total=int(len(test_data)))\n","\n","    model.eval()\n","    preds = []\n","    if mode == \"valid\":\n","        y_true = []\n","    with torch.no_grad():\n","        for batch in test_bar:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            other_features = batch[\"features\"].to(device) if \"features\" in batch else None\n","            logits = model(input_ids, attention_mask, other_features)\n","            predicted = torch.max(logits, dim=1)[1]\n","            preds.extend(predicted.tolist())\n","            if mode == \"valid\":\n","                y_true.extend(batch[\"label\"].tolist())\n","\n","    if mode == \"valid\":\n","        print(classification_report(y_true, preds, digits=4))\n","    else:\n","        review_ids = test_data.dataset.data_file[\"review_id\"]\n","        save_preds(review_ids, np.array(preds), save_name)\n","\n","def save_preds(review_ids, preds, save_name=\"pred.csv\"):\n","    answer_df = pd.DataFrame(data={\n","        'review_id': review_ids,\n","        'stars': preds + 1,\n","    })\n","    answer_df.to_csv(save_name, index=False)"],"metadata":{"id":"AiGJVjN1fkIs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate(model,\n","             val_dataloader,\n","             DEVICE,\n","             mode='valid',\n","             save_name=FLAGS.save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ObmenbKgcqz","executionInfo":{"status":"ok","timestamp":1649511623176,"user_tz":-480,"elapsed":60937,"user":{"displayName":"Hoàn Văn","userId":"09034669333806157547"}},"outputId":"11800642-391c-4c39-8399-9fc8d027d831"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 63/63 [01:00<00:00,  1.04it/s]"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.8421    0.7943    0.8175       282\n","           1     0.4450    0.6250    0.5199       136\n","           2     0.5275    0.5425    0.5349       212\n","           3     0.5144    0.6137    0.5597       466\n","           4     0.8596    0.7312    0.7902       904\n","\n","    accuracy                         0.6855      2000\n","   macro avg     0.6377    0.6613    0.6444      2000\n","weighted avg     0.7133    0.6855    0.6949      2000\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}